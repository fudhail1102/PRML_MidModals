{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZqRl0O-zKTm"
      },
      "source": [
        "\n",
        "*   NumPy: For numerical operations.\n",
        "*   Pandas: For handling datasets.\n",
        "\n",
        "*   Scikit-learn: For machine learning operations, including dataset loading, model training, and evaluation.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IVa-dKBJxF6F"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F10DV91D4igb"
      },
      "source": [
        "\n",
        "\n",
        "## **2. Probability Formula Used**\n",
        "### **Bayes' Theorem**\n",
        "The Naïve Bayes classifier is based on **Bayes’ Theorem**, which is given by:\n",
        "\n",
        "\\[\n",
        "P(Class | Features) = \\frac{P(Features | Class) \\times P(Class)}{P(Features)}\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( P(Class | Features) \\) = Posterior probability (probability of a class given the features).\n",
        "- \\( P(Features | Class) \\) = Likelihood (probability of features given a class).\n",
        "- \\( P(Class) \\) = Prior probability of the class.\n",
        "- \\( P(Features) \\) = Evidence (probability of the features occurring).\n",
        "\n",
        "### **Gaussian Naïve Bayes Assumption**\n",
        "If the features follow a **normal (Gaussian) distribution**, the probability of a feature given a class is calculated as:\n",
        "\n",
        "\\[\n",
        "P(x | Class) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( x \\) = Feature value\n",
        "- \\( \\mu \\) = Mean of the feature in that class\n",
        "- \\( \\sigma^2 \\) = Variance of the feature in that class\n",
        "\n",
        "This assumption allows us to compute the probability of a feature belonging to a class using only its **mean and variance**.\n",
        "\n",
        "### **Final Decision Rule**\n",
        "For classification, we calculate the posterior probability for each class and select the class with the **highest probability**:\n",
        "\n",
        "\\[\n",
        "Class = \\arg\\max P(Class | Features)\n",
        "\\]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JU__FOZ6bVM"
      },
      "source": [
        "## **1. Understanding the Formula**\n",
        "The Naïve Bayes classifier is based on **Bayes' Theorem**, which is used to calculate the probability of a class given certain features.\n",
        "\n",
        "### **Bayes' Theorem:**\n",
        "\\[\n",
        "P(Class | Features) = \\frac{P(Features | Class) \\times P(Class)}{P(Features)}\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( P(Class | Features) \\) = **Posterior probability**, i.e., the probability of a class given the features.\n",
        "- \\( P(Features | Class) \\) = **Likelihood**, i.e., the probability of the features given a class.\n",
        "- \\( P(Class) \\) = **Prior probability** of the class (how often the class appears in the dataset).\n",
        "- \\( P(Features) \\) = **Evidence**, i.e., the probability of the features occurring.\n",
        "\n",
        "### **Naïve Bayes Assumption:**\n",
        "- The algorithm assumes that **features are independent**, meaning each feature contributes to the probability **individually**.\n",
        "- This simplifies calculations and makes Naïve Bayes computationally efficient.\n",
        "\n",
        "### **Gaussian Naïve Bayes (For Continuous Data)**\n",
        "If the features are continuous (numerical), Naïve Bayes assumes they follow a **Gaussian (Normal) Distribution**:\n",
        "\n",
        "\\[\n",
        "P(x | Class) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( x \\) = Feature value\n",
        "- \\( \\mu \\) = Mean of the feature in that class\n",
        "- \\( \\sigma^2 \\) = Variance of the feature in that class\n",
        "\n",
        "### **Classification Decision Rule**\n",
        "To classify a new sample, we calculate the probability for each class and assign the class with the **highest probability**:\n",
        "\n",
        "\\[\n",
        "Class = \\arg\\max P(Class | Features)\n",
        "\\]\n",
        "\n",
        "This means the class with the highest computed probability is chosen as the final prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_2MEzDpxkY7"
      },
      "outputs": [],
      "source": [
        "class NaiveBayesClassifier:\n",
        "    def fit(self, X, y):\n",
        "        \"\"\" Train the Naïve Bayes model by computing priors, means, and variances. \"\"\"\n",
        "        self.classes = np.unique(y)  # Get unique class labels\n",
        "        self.means = {}\n",
        "        self.variances = {}\n",
        "        self.priors = {}\n",
        "\n",
        "        for c in self.classes:\n",
        "            X_c = X[y == c]  # Get all samples belonging to class c\n",
        "            self.means[c] = np.mean(X_c, axis=0)  # Compute mean per feature\n",
        "            self.variances[c] = np.var(X_c, axis=0)  # Compute variance per feature\n",
        "            self.priors[c] = X_c.shape[0] / X.shape[0]  # Compute prior probability\n",
        "\n",
        "    def _gaussian_probability(self, x, mean, var):\n",
        "        \"\"\" Compute Gaussian probability density function \"\"\"\n",
        "        eps = 1e-6  # Small constant to prevent division by zero\n",
        "        coeff = 1 / np.sqrt(2 * np.pi * var + eps)\n",
        "        exponent = np.exp(-((x - mean) ** 2) / (2 * var + eps))\n",
        "        return coeff * exponent\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\" Predict the class for each sample in X \"\"\"\n",
        "        predictions = [self._predict_sample(x) for x in X]\n",
        "        return np.array(predictions)\n",
        "\n",
        "    def _predict_sample(self, x):\n",
        "        \"\"\" Compute probability for each class and return the most probable class \"\"\"\n",
        "        posteriors = []\n",
        "\n",
        "        for c in self.classes:\n",
        "            prior = np.log(self.priors[c])  # Log prior probability\n",
        "            likelihoods = np.sum(np.log(self._gaussian_probability(x, self.means[c], self.variances[c])))\n",
        "            posterior = prior + likelihoods  # Compute log-posterior\n",
        "            posteriors.append(posterior)\n",
        "\n",
        "        return self.classes[np.argmax(posteriors)]  # Return class with highest probability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HxMuau56Oh5"
      },
      "source": [
        "**Types of Naïve Bayes:**\n",
        "* GaussianNB: Assumes features are normally distributed (used in our case).\n",
        "* MultinomialNB: Used for text classification (e.g., spam filtering).\n",
        "* BernoulliNB: Used for binary/boolean features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBm1h--gxs1R"
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDIS26rkz9lb",
        "outputId": "7a1d91c3-ae81-4381-dc42-8cf0fc5f0cea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature Names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
            "Target Names: ['setosa' 'versicolor' 'virginica']\n"
          ]
        }
      ],
      "source": [
        "# Display dataset information\n",
        "print(f\"Feature Names: {iris.feature_names}\")\n",
        "print(f\"Target Names: {iris.target_names}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9D-5-F5xwLc"
      },
      "outputs": [],
      "source": [
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVQUUj5-x1es"
      },
      "outputs": [],
      "source": [
        "# Train Naïve Bayes classifier\n",
        "nb_classifier = NaiveBayesClassifier()\n",
        "nb_classifier.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nc9MjX5Bx4fE"
      },
      "outputs": [],
      "source": [
        "# Make predictions\n",
        "y_pred = nb_classifier.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDANnVg-x7BN",
        "outputId": "694ec528-188b-4c90-e7b8-9dfeb922f9ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Accuracy: 1.00\n"
          ]
        }
      ],
      "source": [
        "# Evaluate performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyLAEGmGx_Tg",
        "outputId": "301739b1-6bed-476f-ccdc-bb7ead3ba371"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      1.00      1.00         9\n",
            "   virginica       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#print(\"Classification Report:\")\n",
        "#print(classification_report(y_test, y_pred, target_names=iris.target_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPtZTIR5zIyk"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
